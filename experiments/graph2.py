import numpy as np
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from data.dataset import create_graph, add_self_loops
from models.gat_model import GATModel, create_transductive_gat, create_inductive_gat
from config import Config

def test_basic_functionality():
    print("ğŸ§ª Testing Basic GAT Functionality")
    print("=" * 50)
    
    adj, features, labels = create_graph(num_nodes=6, num_features=5, num_classes=3, self_loops=True, seed=Config.SEED)
    
    print(f"Graph: {adj.shape[0]} nodes, {features.shape[1]} features, {len(np.unique(labels))} classes")
    from models.gat_layer import GATLayer
    
    gat_layer = GATLayer(in_features=5, out_features=8, seed=Config.SEED)
    output, attention = gat_layer.forward(features, adj, return_attention=True)
    
    print(f"Input shape: {features.shape}")
    print(f"Output shape: {output.shape}")
    print(f"Attention shape: {attention.shape}")
    print(f"Attention row sums: {np.sum(attention, axis=1)[:3]}")  # Should be ~1.0
    
    print("\nğŸ“ Testing Multi-Head GAT Layer:")
    from models.gat_layer import MultiHeadGATLayer
    
    multihead_gat = MultiHeadGATLayer(in_features=5, out_features_per_head=8, n_heads=3, 
                                     concat=True, seed=Config.SEED)
    
    output, attentions = multihead_gat.forward(features, adj, return_attentions=True)
    
    print(f"Multi-head output shape: {output.shape}")
    print(f"Expected shape: (6, {3 * 8})")
    print(f"Number of attention matrices: {len(attentions)}")
    
    return True

test_basic_functionality()